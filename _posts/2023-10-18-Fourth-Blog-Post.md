# Regression Model Selection

An important modeling technique that is very common in statistics is regression. For people not overly familiar with this term [Investopedia](https://www.investopedia.com/terms/r/regression.asp) defines this term as an attempt to determine the strength and character of the relationship between one dependent variable and a series of other independent variables. For example, we might want to predict a relationship between a particular yield for a specific crop and we have information on rainfall and the age of seeds. We could potentially make a regression model with crop yield as the dependent variable and rainfall and age of seeds as the independent variables.

But how do we know if both independent variables in the above example really help us in predicting crop yield? If you have taken a statistics class we might do a hypothesis test and set either or both variables equal to zero and go through with our test to see if we reject or fail to reject our null hypothesis. If we reject our null hypothesis, we tend to keep that independent variable in our regression model. If we fail to reject our null hypothesis (where our p-value we obtain is larger than our significance level $\alpha$ then we tend not to include this in our regression model. Sometimes with less variables this is easy to do. We can put all of our variables into a linear model in our software (for the purpose of this piece I will reference R but other softwares do similar things) and get a summary of each independent variable (in R using `summary(linear_model_created)`) and 
